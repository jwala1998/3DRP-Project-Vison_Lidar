{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPTzEvbjU2Pja1g3/e+gPJG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ld_C_1dcMnKO","executionInfo":{"status":"ok","timestamp":1701872966224,"user_tz":300,"elapsed":18073,"user":{"displayName":"Jwalandhar Girnar","userId":"07935721020492465045"}},"outputId":"49258cf3-0514-4812-a22f-2620f080807c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# from keras.models import Sequential\n","# from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Reshape\n","\n","# # Input shape for a 6-channel image (assuming 160x576 pixels)\n","# input_shape = (90, 298, 3)\n","\n","# # Create a VGG-like model for road recognition\n","# model = Sequential()\n","\n","# # Convolutional Blocks (you can adjust the number of filters, etc., based on your requirements)\n","# model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n","# model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n","# model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n","\n","# model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n","# model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n","# model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n","\n","# model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n","# model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n","# model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n","# model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n","# model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n","\n","# # Flatten and Fully Connected Layers\n","# model.add(Flatten())\n","# model.add(Dense(1024, activation='relu'))\n","# model.add(Dense(512, activation='relu'))\n","# model.add(Dense(160 * 576, activation='sigmoid'))  # Output layer for binary road mask\n","# model.add(Reshape((160, 576, 1)))\n","\n","# # model.add(Dense(90 * 298, activation='sigmoid'))  # Output layer for binary road mask\n","# # model.add(Reshape((90, 298, 1)))\n","\n","# # Compile the model\n","# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# # Display the model summary\n","# model.summary()\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YwVJM4_vMwkK","executionInfo":{"status":"ok","timestamp":1701873626875,"user_tz":300,"elapsed":3041,"user":{"displayName":"Jwalandhar Girnar","userId":"07935721020492465045"}},"outputId":"b31b2f98-e7a1-4dc4-b5ce-de6703ab51cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_8 (Conv2D)           (None, 90, 298, 64)       1792      \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 90, 298, 64)       36928     \n","                                                                 \n"," max_pooling2d_3 (MaxPoolin  (None, 45, 149, 64)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 45, 149, 128)      73856     \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 45, 149, 128)      147584    \n","                                                                 \n"," max_pooling2d_4 (MaxPoolin  (None, 22, 74, 128)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_12 (Conv2D)          (None, 22, 74, 256)       295168    \n","                                                                 \n"," conv2d_13 (Conv2D)          (None, 22, 74, 256)       590080    \n","                                                                 \n"," conv2d_14 (Conv2D)          (None, 22, 74, 256)       590080    \n","                                                                 \n"," conv2d_15 (Conv2D)          (None, 22, 74, 256)       590080    \n","                                                                 \n"," max_pooling2d_5 (MaxPoolin  (None, 11, 37, 256)       0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_1 (Flatten)         (None, 104192)            0         \n","                                                                 \n"," dense_3 (Dense)             (None, 1024)              106693632 \n","                                                                 \n"," dense_4 (Dense)             (None, 512)               524800    \n","                                                                 \n"," dense_5 (Dense)             (None, 92160)             47278080  \n","                                                                 \n"," reshape_1 (Reshape)         (None, 160, 576, 1)       0         \n","                                                                 \n","=================================================================\n","Total params: 156822080 (598.23 MB)\n","Trainable params: 156822080 (598.23 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","input_shape = (90, 298, 4)\n","model = Sequential()\n","\n","# Convolutional layers\n","model.add(layers.Conv2D(32, (3, 3), activation='relu',padding='same', input_shape=input_shape))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","\n","# Upsampling layers\n","model.add(layers.UpSampling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model.add(layers.UpSampling2D((2, 2)))\n","model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n","\n","# Output layer\n","model.add(layers.Conv2D(1, (1, 1), activation='sigmoid'))\n","\n","\n","\n","# Instantiate the model\n","# model = road_detection_model()\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Display the model summary\n","model.summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7TQpFVlk-GNB","executionInfo":{"status":"ok","timestamp":1701873936809,"user_tz":300,"elapsed":291,"user":{"displayName":"Jwalandhar Girnar","userId":"07935721020492465045"}},"outputId":"0f1532f7-580d-4a86-f5c4-4b3fbe70132d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_22 (Conv2D)          (None, 90, 298, 32)       1184      \n","                                                                 \n"," max_pooling2d_8 (MaxPoolin  (None, 45, 149, 32)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_23 (Conv2D)          (None, 43, 147, 64)       18496     \n","                                                                 \n"," max_pooling2d_9 (MaxPoolin  (None, 21, 73, 64)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_24 (Conv2D)          (None, 19, 71, 128)       73856     \n","                                                                 \n"," up_sampling2d_2 (UpSamplin  (None, 38, 142, 128)      0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_25 (Conv2D)          (None, 36, 140, 64)       73792     \n","                                                                 \n"," up_sampling2d_3 (UpSamplin  (None, 72, 280, 64)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_26 (Conv2D)          (None, 70, 278, 32)       18464     \n","                                                                 \n"," conv2d_27 (Conv2D)          (None, 70, 278, 1)        33        \n","                                                                 \n","=================================================================\n","Total params: 185825 (725.88 KB)\n","Trainable params: 185825 (725.88 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["#preprocessing\n","import os\n","import cv2\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Reshape\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","\n","# Function to load and preprocess the data\n","def load_and_preprocess_data(training,gt):\n","    images = []\n","    masks = []\n","\n","    # img_0= [f for f in os.listdir(training) if f.endswith(\".npy\")]\n","    img_0= [f for f in os.listdir(training) if f.endswith(\".png\")]\n","\n","    test_img_0= [f for f in os.listdir(gt) if f.endswith(\".png\")]\n","\n","    img_0.sort()\n","    test_img_0.sort()\n","    # print(img_0)\n","\n","    for img, test_img in zip(img_0,test_img_0):\n","    # Construct the full paths\n","      img_path= os.path.join(training, img)\n","\n","      # img_1=np.load(img_path)\n","      img_1=cv2.imread(img_path)\n","\n","      img_1= cv2.resize(img_1,(298,90))\n","      test_img_path= os.path.join(gt, test_img)\n","\n","      test_image= cv2.imread(test_img_path)\n","      test_image=cv2.resize(test_image,(576,160))\n","      # test_image=cv2.resize(test_image,(278,70))\n","\n","\n","      # cv2_imshow(test_image)\n","\n","      lower_green = np.array([255, 0, 255], dtype=np.uint8)\n","      upper_green = np.array([255, 0, 255], dtype=np.uint8)\n","\n","      # Create a mask using inRange function to filter out green pixels\n","      mask = cv2.inRange(test_image, lower_green, upper_green)\n","\n","      images.append(img_1)\n","      masks.append(mask)\n","\n","    images = np.array(images, dtype='float32')\n","    masks = np.array(masks, dtype='float32')  # Assuming binary masks\n","\n","    return images, masks\n","\n","    # for filename in os.listdir(os.path.join(data_folder, 'images')):\n","    #     img_path = os.path.join(data_folder, 'images', filename)\n","    #     mask_path = os.path.join(data_folder, 'masks', filename.replace('.png', '_mask.png'))\n","\n","    #     # img = cv2.imread(img_path)\n","    #     # img = cv2.resize(img, (576, 160))  # Resize images to match the model input size\n","\n","    #     img = img / 255.0  # Normalize pixel values to [0, 1]\n","\n","    #     mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","    #     mask = cv2.resize(mask, (576, 160))\n","    #     mask = (mask > 0).astype(np.uint8)  # Convert to binary mask\n","\n","    #     images.append(img)\n","    #     masks.append(mask)\n","\n","    # images = np.array(images, dtype='float32')\n","    # masks = np.array(masks, dtype='float32')  # Assuming binary masks\n","\n","    # return images, masks\n","\n","# Load and preprocess the data\n","# training_folder = 'drive/My Drive/3DRP project/Project/data_road/training/image_2_6channel' # Change this to the path of your data folder\n","training_folder = 'drive/My Drive/3DRP project/Project/data_road/testing/image_2' # Change this to the path of your data folder\n","\n","gt_folder= 'drive/My Drive/3DRP project/Project/data_road/testing/gt_image_2'\n","images, masks = load_and_preprocess_data(training_folder,gt_folder)\n","\n","masks.shape\n","# os.listdir(training_folder)\n","# Split the data into training and validation sets\n","# train_images, val_images, train_masks, val_masks = train_test_split(images, masks, random_state=42)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4VW3QlPHSjqp","executionInfo":{"status":"ok","timestamp":1701873643338,"user_tz":300,"elapsed":1971,"user":{"displayName":"Jwalandhar Girnar","userId":"07935721020492465045"}},"outputId":"c7b4fc2e-8e17-45a4-a859-e964baf3f784"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60, 160, 576)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["images.shape\n","# cv2_imshow(masks[2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":177},"id":"N7gSRfGiw90k","executionInfo":{"status":"ok","timestamp":1701829230808,"user_tz":300,"elapsed":171,"user":{"displayName":"Jwalandhar Girnar","userId":"07935721020492465045"}},"outputId":"2de3992f-bfdd-452c-ea96-461f09fdc40e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=576x160>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAkAAAACgCAAAAADBKoxBAAAC90lEQVR4nO3TOZLQQBAF0SSC+18ZDDCGZUYtqZda8nnyun6kQJIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZIkSZI05NvpB4Ty489Px7nmRvBPOB+4zwUH+iofHOiK+3yZDy50ofs8V/ngRF/rvc5APnTf6ELnccbyofdIV9puM1wPNF7pWs9pbtUDXWca0XGZ2/nQc6ch7YZ5Ug80HGpQr12e1gN0m2pUp1Ve5UOvrYb1GeVtPnQaa1yPTSbE80uPue7osMi0fOix1y3lB5lZDzQY7Kbae8yuB6ovdlvlOVbkQ+3J7iu7xqJ6gMKjPVB0i5X5UHa1JypOsbgeqDnbM+WW2FAPFNztqWJDbMqHcsM9VmmHffVAreVeKDPD3nqAQtu9UWOEA/VAlfHeqbDBoXyosd5L6Sc4Vw8UmO+13AucrQey7zdB5gHO50PuAWdIe3+IeoDEE06R9Po4+ZB2wzm+n37AA6Hq6S7d3xOynnQrzpPs9JD5kG7GiTJdHrUeyLXjVGkOj1wPJBpyshx3R68Hsiw5XYazM+RDjinnC391knogwZYrxD46UT0Qfcw1It+cLB9ir7lI2JPz1QME3nOVoAcnzYewgy4T8d689UDMRRcKd27ueiDgpEvFujZ/PRBt08UCHVujHgg16nJhbq2TD4FWXS/GqaXqgSiz7hDg0nL1QIhd9zh+aMl8CDDsJmfvrFoPnF52m4NnVq4HaJLQqSPL1wM9CjpzY4t8aFHQgRO71AMdCtp+Yad8aFDQ3gOb1QP1C9p4X8N6oHxB285rmg/VC9pzXd96oHhBG47rXQ/ULmj1bdYDpQtae5r5/Fa3oIWXWc9HVRNadZf1/K1oQWvOMp//qFnQgqus5xMlC5p+lPl8rmJBc2+ynq8VLGjiSdZzrV5B0y4ynyHlCppzkPUMq1bQhHus55ZiBb09x3puq1VQpGu6xBhpc+msnwhdTlEi4BGoAAAAAElFTkSuQmCC\n"},"metadata":{}}]},{"cell_type":"code","source":["# Split the data into training and validation sets\n","train_images, val_images, train_masks, val_masks = train_test_split(images, masks, random_state=42)\n","train_images.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pWS3Jirauor_","executionInfo":{"status":"ok","timestamp":1701873653340,"user_tz":300,"elapsed":215,"user":{"displayName":"Jwalandhar Girnar","userId":"07935721020492465045"}},"outputId":"5b0a4b69-be53-494d-9ecd-bfffdd60570a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(45, 90, 298, 3)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# cv2_imshow(val_images[5])\n"],"metadata":{"id":"y1upZAR43RGl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","tf.config.run_functions_eagerly(True)\n","from keras import models"],"metadata":{"id":"jmEK8KlG0P3R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Train the model in batches\n","batch_size = 8\n","epochs = 5\n","\n","steps_per_epoch = len(train_images) // batch_size\n","model.fit(train_images, train_masks, validation_data=(val_images, val_masks), epochs=epochs, batch_size=batch_size, steps_per_epoch=steps_per_epoch)\n","\n","\n","# model.fit(train_images, train_labels, validation_data=(val_images, val_labels), epochs=epochs, batch_size=batch_size)\n","\n","# Save the trained model\n","model.save('road_recognition_model.h5')\n","\n","trained_model = models.load_model('road_recognition_model.h5')\n","\n","# Predict on validation data\n","predictions = trained_model.predict(val_images)\n","\n","# Threshold the predictions to obtain binary masks\n","threshold = 0.5\n","binary_predictions = (predictions > threshold).astype(np.uint8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":465},"id":"arjXO6h_MtrU","outputId":"c8eee2cf-fe1b-45d5-9c66-da56d2965830","executionInfo":{"status":"error","timestamp":1701873942427,"user_tz":300,"elapsed":253,"user":{"displayName":"Jwalandhar Girnar","userId":"07935721020492465045"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n","  warnings.warn(\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-7a07cafd54b5>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m    299\u001b[0m                             \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                             \u001b[0;34m\"incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 90, 298, 4), found shape=(8, 90, 298, 3)"]}]},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow\n","\n","i=5\n","\n","# original_image = (val_images[i] * 255).astype(np.uint8)\n","ground_truth_mask = (val_masks[i] )\n","predicted_mask = (binary_predictions[i] )\n","\n","# Concatenate the original image, ground truth mask, and predicted mask horizontally\n","# result_image = np.concatenate([original_image, ground_truth_mask, predicted_mask], axis=1)\n","\n","# Display or save the result image\n","# cv2_imshow( original_image)\n","cv2_imshow(ground_truth_mask)\n","# cv2_imshow(predicted_mask)\n","\n","cv2.waitKey(0)  # Press any key to move to the next image\n","\n","cv2.destroyAllWindows()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":177},"id":"-qxPC-ya1EDP","executionInfo":{"status":"ok","timestamp":1701873483562,"user_tz":300,"elapsed":174,"user":{"displayName":"Jwalandhar Girnar","userId":"07935721020492465045"}},"outputId":"5cf9d82c-8520-4fe6-8322-f6ab16654d01"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=576x160>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAkAAAACgCAAAAADBKoxBAAADEklEQVR4nO3dy3IbMRBDUSSV///lZKFVHEcakk2yG33PzjsBhXlpVGUJAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACb8uP0BMvn95W/K+ezX7Q+Qx9f54Imftz9AGt/sh0l9xhlIElOZx4DEfFZwn/h+PvTzQfeCnpx8unf0Vu9LGNeuZY2PrufraVzSR027GT31NK3pgZbNTFy5Wvb0RMNi5m58Ghb1SLNeVu6am1X1UKenMJ65NuhzWK3Pp09XA3qUEnXu6dHWkA6VBF66OtQ1xr6R4Bsf+75GeRey4bbZu7Bxxn1seugybmyGaR1bn9hNO5tjWcbuL3wsS5vk18WJrwv9WptmVsWpL5vNalvg1MTJVxVOvS3xKeLwmy6f4tZ49HDjNalHc8scarj1lt2hu2XlS7j5G43y5QUo3cH1H/iUbi9G4Qquz0el6wtStYEM65Hq9hemYgFZxiOpZoGR6uVPNR9VbDBUtfjZ5qN6FcaqlD7heF4qlRitTPa065EKtRivSPTU81GZGjcokDz7eCSV6HGP7MFLrEfKX+QuuXOXmY+yN7lN3tiVxiMpc5U7JU1dbj1S2i73Shm65HykpG3ulS5y2fFISljndskS156P0vW5X6bA5dcj5Sr0hDR5LdYjJWr0jBxxbdYjZan0lARprdYjpej0nNth7dYj6X6rB92N6jkf3a71pGtJbbfz0mZBl4Kaz0d9FnQlp/981GZBx2O2GM9LiwmdDdloPVKPBZ3M2Gw+arGgUxH7jUdShwUdSdh0PVKDBR0I2Hg+8l/Q5ny9x/PiPaGt6ZiPJPMF7fuPhaynhU1HB+v5i/E5aEc01vMP3wWFJ2M937JdUGww1vN/phMKjMV63vNcUNhTGPPpKeSwYDzPOJ6DAjIxn8cMF7QaifUM8VvQSiLGM8FtQvM30cwHmj4gWM80s1PQRBzGs8hqQsNhmM86pwWNZWE9MYwWNBCF9cTxWdDTJKwnls+CAOCaP7YoUmHIP2dOAAAAAElFTkSuQmCC\n"},"metadata":{}}]}]}